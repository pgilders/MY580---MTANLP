{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b69434",
   "metadata": {},
   "source": [
    "# üß± Foundations\n",
    "\n",
    "We will be working with [spaCy](https://spacy.io/), a Python library for Natural Language Processing (NLP). It is fast and easy to use. It has pre-trained models for various languages and can be used for tasks like tokenization, part-of-speech tagging, named entity recognition, and more. [NLTK](https://www.nltk.org/) is another popular, slightly more traditional, text analysis / NLP library which we make brief use of.\n",
    "\n",
    "Make sure to check out the spaCy documentation for more details on how to use it: [spaCy Documentation](https://spacy.io/usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If spaCy or the English model is not installed, install them:\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # load English tokenizer, POS tagger, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b2804",
   "metadata": {},
   "source": [
    "`\"en_core_web_sm\"` is one of spaCy's pre-trained model pipelines. It is a combination of different machine learning and rule based components that can be used as tools to process text in different ways. The `\"en_core_web_sm\"` model is a small English model that is fast and efficient, but not as accurate as larger models like `\"en_core_web_md\"` or `\"en_core_web_lg\"`, or transformer based models.\n",
    "\n",
    "* en ‚Äì English language\n",
    "* core ‚Äì General-purpose (not domain-specific)\n",
    "* web ‚Äì Trained primarily on web data (blogs, news, comments)\n",
    "* sm ‚Äì \"Small\" size model (~13MB)\n",
    "\n",
    "You can use alternative models, provided by spaCy, other users, or create your own. We'll stick with this for some basic tasks for now, and scale up later in the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1f936",
   "metadata": {},
   "source": [
    "## üßπ Preprocessing\n",
    "\n",
    "Preprocessing is a crucial step in text analysis and NLP that involves cleaning and transforming raw text data into a format suitable for analysis. This step is essential for working with traditional statistical models to deep learning architectures.\n",
    "\n",
    "### ü§î Why Preprocessing Matters\n",
    "\n",
    "Proper preprocessing can significantly improve the performance of text analysis and some NLP models:\n",
    "\n",
    "* It reduces noise and irrelevant variation (e.g., unifying casing, removing filler words).\n",
    "* It normalizes text so that different forms of the same word are treated equally (‚Äúrunning‚Äù vs ‚Äúran‚Äù ‚Üí ‚Äúrun‚Äù).\n",
    "* It can reduce dimensionality (fewer unique tokens after cleaning), which is important for traditional ML models.\n",
    " \n",
    "However, be mindful: aggressive preprocessing (like removing too many words or context via stemming) can sometimes hurt performance, especially for deep learning models that might prefer raw text with context. So use preprocessing appropriately for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7735d",
   "metadata": {},
   "source": [
    "### üß∞ Common Preprocessing Steps\n",
    "\n",
    "The following are common preprocessing steps used. Depending on the task, model, and data, you you should choose the appropriate steps. Some steps may be more relevant for certain tasks than others, and some may not be necessary at all (or even counterproductive).\n",
    "\n",
    "1. **Lowercasing**: Convert all text to lowercase to ensure uniformity.\n",
    "2. **Tokenization**: Split text into individual words or tokens.\n",
    "3. **Removing Punctuation**: Strip punctuation marks from the text to focus on the words.\n",
    "4. **Removing Stop Words**: Eliminate common words (like \"the\", \"is\", \"and\") that may not add significant meaning to the text. Libraries like NLTK and SpaCy provide lists of stop words.\n",
    "5. **Stemming/Lemmatization**: Reduce words to their base or root form. Stemming is a more aggressive approach that may not always yield valid words, while lemmatization uses a dictionary to ensure the root word is valid.\n",
    "6. **Removing Numbers**: Depending on the context, you may want to remove numbers from the text.\n",
    "7. **Removing Extra Whitespace**: Clean up any extra spaces or newlines in the text.\n",
    "8. **Handling Special Characters**: Depending on the dataset, you may need to remove or replace special characters (like emojis or HTML tags).\n",
    "9. **Handling Contractions**: Expand contractions (e.g., \"don't\" to \"do not\") to ensure uniformity.\n",
    "10. **Handling URLs and Emails**: Depending on the context, you may want to remove or replace URLs and email addresses.\n",
    "11. **Handling Case Sensitivity**: Depending on the task, you may want to keep the original casing of words (e.g., for named entity recognition) or convert everything to lowercase.\n",
    "12. **Handling Synonyms and Antonyms**: Depending on the task, you may want to replace synonyms or antonyms with a common term (e.g., \"happy\" and \"joyful\" to \"happy\").\n",
    "13. **Handling Negations**: Depending on the task, you may want to handle negations (e.g., \"not happy\" to \"unhappy\") to ensure the model understands the sentiment.\n",
    "14. **Handling Domain-Specific Terms**: Depending on the dataset, you may need to handle domain-specific terms (e.g., medical terms, technical jargon) to ensure the model understands the context.\n",
    "15. **Handling Language-Specific Features**: Depending on the language, you may need to handle specific features (e.g., accents, diacritics) to ensure the model understands the text.\n",
    "16. **Handling Multilingual Text**: If your dataset contains multiple languages, you may need to preprocess each language differently or use a multilingual model.\n",
    "17. **Handling Code and Programming Language Syntax**: If your dataset contains code snippets or programming language syntax, you may need to preprocess it differently to ensure the model understands the context.\n",
    "18. **Handling Text Length**: Depending on the model, you may need to truncate or pad text to a specific length to ensure uniformity.........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c20da3",
   "metadata": {},
   "source": [
    "Let‚Äôs take a sample sentence and perform some basic preprocessing with spaCy. We‚Äôll use a simple example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7841d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "doc = nlp(text) # spaCy tokenises and processes the text\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", [token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa70650",
   "metadata": {},
   "source": [
    "We could have manually tokenized the text, e.g. by splitting on whitespace, punctuation, but this rapidly becomes a non trivial task. spaCY's tokenizer is a powerful tool that can handle a wide variety of tokenization tasks, including splitting on whitespace, punctuation, and special characters. It also handles edge cases like contractions, abbreviations, and special characters. Notice how spaCy handled ‚ÄúU.K.‚Äù as one token and split ‚Äú\\$1 billion‚Äù into \"$\", \"1\", and \"billion\" separately. It also keeps punctuation (.) as a token by default. It can also be configured with custom rules to handle specific cases, such as splitting on certain characters or keeping certain tokens together, or even a completely different tokenization algorithm. This makes it a powerful and flexible tool for tokenization in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267d49b",
   "metadata": {},
   "source": [
    "spaCy‚Äôs Doc object also contains rich linguistic annotations. We can iterate through doc to get **lemmas**, **part-of-speech (POS) tags**, and **other attributes**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8674ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \"‚Üí lemma:\", token.lemma_, \"| POS:\", token.pos_, \"| StopWord?\", token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225bc79",
   "metadata": {},
   "source": [
    "Here we see:\n",
    "\n",
    "* Lemmatization: ‚Äúis‚Äù became ‚Äúbe‚Äù; ‚Äúbuying‚Äù became ‚Äúbuy‚Äù. Lemmas are useful to reduce inflected forms to a common base.\n",
    "* Part-of-speech tags: ‚ÄúApple‚Äù is a proper noun (PROPN), ‚Äúis‚Äù is an auxiliary verb (AUX), ‚Äúbuying‚Äù a verb, etc.\n",
    "* Stop words: spaCy has a built-in list of stop words for English. It flagged ‚Äúis‚Äù, ‚Äúat‚Äù, ‚Äúfor‚Äù as stop words (common function words). ‚ÄúApple‚Äù and ‚Äúbuying‚Äù are not stop words.\n",
    "\n",
    "Using these annotations, we can easily perform typical preprocessing tasks. For instance, to remove stop words and punctuation and get a list of normalized tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token.lemma_.lower() for token in doc \n",
    "          if not token.is_stop and not token.is_punct]\n",
    "print(\"Processed tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417c6e4",
   "metadata": {},
   "source": [
    "We lowercased all tokens and removed common stop words (‚Äúis‚Äù, ‚Äúat‚Äù, ‚Äúfor‚Äù) and punctuation. The result is a crude but useful representation of the \"content\" words in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b527fa10",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c00cfd",
   "metadata": {},
   "source": [
    "1. Try changing the example text or adding another sentence and re-running the above code. For instance, what happens if you include a contraction or a numerical date? Examine how spaCy tokenizes and lemmatizes it. You can also try printing other token attributes like token.dep_ (dependency relation) to see syntactic dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95519f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b68ad3a4",
   "metadata": {},
   "source": [
    "2. The spaCy pipeline we used includes POS tagging, lemmatization, etc. If you wanted to add a custom preprocessing step (say, replacing all numbers with a special token like \\<NUM>). This is useful for tasks like text classification where the actual number may not be important but the fact that there is a number is. How might you do it?\n",
    "\n",
    "*Hint: You could post-process the token list or use regex on the original text before sending to spaCy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f4fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "762aceab",
   "metadata": {},
   "source": [
    "## üìö Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd90dc0",
   "metadata": {},
   "source": [
    "Imagine we now have a corpus of text documents (e.g. news headlines). We can use spaCy to process each document in the corpus and extract useful features for analysis. For example, we can create a **bag-of-words** representation of the corpus, where each document is represented as a vector of word counts or term frequencies. This is a common preprocessing step for traditional machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_headlines = [\n",
    "    \"Apple stocks soar to record highs as investors celebrate strong earnings\",\n",
    "    \"Global markets crash amid fears of a deepening recession and economic turmoil\",\n",
    "    \"Central bank raises interest rates, boosting confidence in economic recovery\",\n",
    "    \"Tesco reports impressive quarterly profits as retail sales surge\",\n",
    "    \"Oil prices collapse after oversupply announcement, sparking industry panic\",\n",
    "    \"Unemployment rate drops to lowest level in a decade as job market thrives\",\n",
    "    \"Debenhams files for bankruptcy protection as retail sector faces disaster\",\n",
    "    \"Investors optimistic as trade tensions ease and markets rally strongly\",\n",
    "    \"Housing market slows down as prices fall and buyers worry\",\n",
    "    \"Novo Nordisk faces lawsuit over drug safety, shares plunge on negative news\",\n",
    "    \"Consumer confidence reaches all-time high as retail sales boom\",\n",
    "    \"Tesla recalls thousands of vehicles over safety concerns, shares tumble\",\n",
    "    \"Earnings miss sends shares of Trump Inc plummeting as profits disappoint\",\n",
    "    \"Mergers and acquisitions activity hits new peak as companies celebrate growth\",\n",
    "    \"Cryptocurrency market rebounds after sharp decline, investors cheer recovery\",\n",
    "    \"Retail sales disappoint during holiday season as consumer confidence drops\",\n",
    "    \"Government stimulus package boosts economic outlook, markets surge\",\n",
    "    \"Manufacturing sector contracts for third straight month as demand weakens\",\n",
    "    \"Lab grown meat startup secures major funding round, industry hails innovation\",\n",
    "    \"Layoffs announced as company restructures operations, employees face uncertainty\"\n",
    "]\n",
    "\n",
    "corpus = list(nlp.pipe(financial_headlines))\n",
    "print(\"Number of documents in the corpus:\", len(corpus))\n",
    "for doc in corpus:\n",
    "    print(\"Tokens:\", [token.text for token in doc])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627c0db",
   "metadata": {},
   "source": [
    "### üßÆ Document-Feature Matrices (DFMs)\n",
    "\n",
    "We can view this in a **document-feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import count vectorizer and pandas to display the results\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocess texts with spaCy (lemmatize, remove stopwords/punctuation)\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Use CountVectorizer with the spaCy tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer)\n",
    "X = vectorizer.fit_transform(financial_headlines)\n",
    "\n",
    "# Convert to dense matrix and view as DataFrame\n",
    "df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names_out()),\n",
    "                  index=financial_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cf73d",
   "metadata": {},
   "source": [
    "The dataframe displays a count of each token in the corpus for each document. The columns represent the words in the vocabulary, and the rows represent the documents. Each row is a numerical vector that can be used to describe that document.\n",
    "\n",
    "The token count method is a simple way to create a document-feature matrix, but there are many other ways to represent text data, such as using term frequency-inverse document frequency (TF-IDF) or word embeddings. These methods can capture more complex relationships between words and documents, and are often used in more advanced NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4914a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559426b1",
   "metadata": {},
   "source": [
    "There are some repeated words (below), but overall the matrix is sparse. This is typical for text data, where most words are not present in most documents. We can also see that some words appear more frequently than others (e.g. \"market\" appears in several documents). This is common in text data, where some words are more informative than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54bb912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8e89b",
   "metadata": {},
   "source": [
    "Visualise with a wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e16d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_tokens = [token.lemma_.lower() for doc in corpus for token in doc\n",
    "              if not token.is_stop and not token.is_punct]  # take all tokens from all documents that we processed with spaCy\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(all_tokens))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362907d",
   "metadata": {},
   "source": [
    "I actually think wordclouds are a bit of a gimmick, but they can be useful for quickly visualising the most common words in a corpus. The larger the word, the more frequently it appears in the corpus. This can help you identify common themes or topics in the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b21715",
   "metadata": {},
   "source": [
    "## üòÄ Sentiment Analysis\n",
    "\n",
    "We can do some simple dictionary-based sentiment analysis to assess how positive or negative the headlines are. This is a simple approach that uses a predefined list of positive and negative words (or n-grams) to score the sentiment of each document. The score is the difference between the positive and negative token in the document. This is a very basic approach, but it can be useful for quickly assessing the sentiment of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce246e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load VADER\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyse each text\n",
    "for doc in corpus:\n",
    "    score = vader.polarity_scores(doc.text)\n",
    "    print(f\"Text: {doc.text}\")\n",
    "    print(f\"VADER Scores: {score}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec4dba",
   "metadata": {},
   "source": [
    "This works \"OK\" on this data. Firstly the documents are short, so the scores are very sensitive to the presence of a few positive or negative words. Secondly, the sentiment lexicon is not perfect - VADER is designed for social media text and conversational language. This is a common problem with dictionary-based approaches, which rely on a predefined list of words and may not capture all the nuances of the text. We could use a better specified dictionary, or one of the more advanced methods are available in spaCy (see later...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63929b0f",
   "metadata": {},
   "source": [
    "## üë§ Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a common task in NLP that involves identifying and classifying \"named entities\" in text. Named entities can include people, organizations, locations, dates, and other specific terms. NER is useful for extracting structured information from unstructured text data. We don't have time today, but I'd recommend also looking at the docs for Entity Linking. We use the `en_core_web_sm` model, which has a built-in NER component that can recognize and classify named entities in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728195ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"Barack Obama was the 44th President of the United States, serving from 2009 to 2017. \"\n",
    "        \"A member of the Democratic Party, he previously served as a U.S. Senator from Illinois from 2005 to 2008. \"\n",
    "        \"Obama was born on August 4, 1961, in Honolulu, Hawaii.\")\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"‚Üí\", ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f2bae0",
   "metadata": {},
   "source": [
    "The model correctly identified Barack Obama as a **PERSON**, United States as a **GPE** (geo-political entities), the years and the full date as **DATE** entities, Democratic Party as an **ORG** (organization), U.S. Senator as an **ORG** (some models label titles or certain political offices as **ORG**), and locations like Illinois, Honolulu, Hawaii as **GPE**. Notably, \"44th\" was labeled **ORDINAL** (as in 44th President)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07d81d",
   "metadata": {},
   "source": [
    "Let's scale things up, considering a book about a popular wizard by some no-name author. You can download the data [here](https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books/data). Move the file to the `data` folder in this project fodler and rename it `wizard_book.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b1ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "with open('data/wizard_book.txt', 'r') as file:\n",
    "    booktext = file.read()\n",
    "\n",
    "# Process the text with spaCy\n",
    "bookdoc = nlp(booktext)\n",
    "print(\"Number of tokens in the book:\", len(bookdoc))\n",
    "print(\"Number of sentences in the book:\", len(list(bookdoc.sents)))\n",
    "print(\"Number of named entities in the book:\", len(bookdoc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the entites mentioned most frequently\n",
    "ents = pd.Series([ent.text for ent in bookdoc.ents])\n",
    "display(ents.value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c00ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network of the entities based on their co-occurrence in the same sentence\n",
    "entity_sets = []\n",
    "for sent in bookdoc.sents:\n",
    "    entities = set(sent.ents)\n",
    "    if len(entities) > 1:\n",
    "        entity_sets.append(entities) \n",
    "\n",
    "# Each item in entity_sets is the group of entities that co-occur in the same sentence\n",
    "entity_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee52587",
   "metadata": {},
   "source": [
    "Let's create an edgelist and plot as a network. This is a simple way to visualize the relationships between entities in the text. One could also use social network analysis techniques to analyze the relationships between entities, such as centrality measures or community detection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary of co-occurring entity pairs and their counts\n",
    "edges = {}\n",
    "for entity_set in entity_sets:\n",
    "    # For each unique pair of entities in the set\n",
    "    for n, entity1 in enumerate(entity_set):\n",
    "        for m, entity2 in enumerate(entity_set):\n",
    "            # Only consider each pair once and skip self-pairs\n",
    "            if (m > n) and (entity1.text != entity2.text):\n",
    "                # If the pair already exists (in either order), increment the count\n",
    "                if (entity1.text, entity2.text) in edges:\n",
    "                    edges[(entity1.text, entity2.text)] += 1\n",
    "                elif (entity2.text, entity1.text) in edges:\n",
    "                    edges[(entity2.text, entity1.text)] += 1\n",
    "                else:\n",
    "                    # Otherwise, initialize the count for this pair\n",
    "                    edges[(entity1.text, entity2.text)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2356c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(edges, orient='index', columns=['weight'])\n",
    "\n",
    "# Reset index and expand tuple into separate columns\n",
    "df = df.reset_index()\n",
    "df[['entity1', 'entity2']] = pd.DataFrame(df['index'].tolist(), index=df.index)\n",
    "df = df[['entity1', 'entity2', 'weight']]\n",
    "df = df.sort_values(by='weight', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648212b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a graph from the top 500 edges\n",
    "G = nx.from_pandas_edgelist(df.head(500), 'entity1', 'entity2', ['weight'])\n",
    "\n",
    "# Draw the graph with edge weights\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.5)\n",
    "\n",
    "# Get edge weights for width scaling\n",
    "edges_ = G.edges(data=True)\n",
    "weights = [d['weight'] for (_, _, d) in edges_]\n",
    "# Normalize weights for plotting (optional: adjust scaling as needed)\n",
    "max_weight = max(weights) if weights else 1\n",
    "widths = [0.1 + 6 * (w / max_weight) for w in weights]\n",
    "\n",
    "nx.draw(\n",
    "    G, pos,\n",
    "    with_labels=True,\n",
    "    node_size=10,\n",
    "    font_size=10,\n",
    "    font_weight='bold',\n",
    "    edge_color='gray',\n",
    "    width=widths\n",
    ")\n",
    "plt.title(\"Entity Co-occurrence Network (Edge Width = Weight)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff24d02",
   "metadata": {},
   "source": [
    "It's a little bit of a mess so far, but we can already see some interesting relationships between the entities and patterns in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a35c4",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Exercise\n",
    "\n",
    "1. How could we improve this network? What are we actually interested in?\n",
    "2. Implement these changes and re-run the code. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac39d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5b8af8",
   "metadata": {},
   "source": [
    "## üåØ Wrap-up\n",
    "\n",
    "We've introduced some basic concepts in text analysis and NLP, including preprocessing, document-feature matrices, sentiment analysis, and named entity recognition.\n",
    "\n",
    "You might have realised by this stage that (pre-)processing is pervasive. It can also be part art part science. There are different ways and points at which you can preprocess text data, and this will be sensitive to the task, data, and model you are using.\n",
    "\n",
    "Finally, a word on validation. So far we have mostly just applying models, without rigorously validating that they are working well on our data (e.g. in sentiment analysis, NER). How do we know how well the sentiment of our headlines being captured, or how many of the named entities are being correctly identified? We should always firstly sense-check our results, then more formall;y validate (e.g. compare against some manual human labelling). More on this later..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
