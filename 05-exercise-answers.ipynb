{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f0cce4",
   "metadata": {},
   "source": [
    "# üôã Answers\n",
    "\n",
    "These won't run within this notebook, you should copy / adapt them to the appropriate location within the other materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c071c787",
   "metadata": {},
   "source": [
    "## üß± Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841deb26",
   "metadata": {},
   "source": [
    "1. Try changing the example text or adding another sentence and re-running the above code. For instance, what happens if you include a contraction or a numerical date? Examine how spaCy tokenizes and lemmatizes it. You can also try printing other token attributes like token.dep_ (dependency relation) to see syntactic dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abfe6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion. It doesn't know whether the deal will go through by 10th November.\"\n",
    "doc = nlp(text) # spaCy tokenises and processes the text\n",
    "print(\"Tokens:\", [token.text for token in doc])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"‚Üí lemma:\", token.lemma_, \"| POS:\", token.pos_, \"| StopWord?\", token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89ff4c",
   "metadata": {},
   "source": [
    "2. The spaCy pipeline we used includes POS tagging, lemmatization, etc. If you wanted to add a custom preprocessing step (say, replacing all numbers with a special token like \\<NUM>). This is useful for tasks like text classification where the actual number may not be important but the fact that there is a number is. How might you do it?\n",
    "\n",
    "*Hint: You could post-process the token list or use regex on the original text before sending to spaCy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "# Replace numbers with a placeholder\n",
    "# This is useful for tasks like text classification where the actual number may not be important\n",
    "# but the fact that there is a number is.\n",
    "\n",
    "tokens_with_num = [\"<NUM>\" if token.like_num else token.text for token in doc]\n",
    "print(\"Tokens with numbers replaced:\", tokens_with_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2028b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer: filter to only include PERSON entities\n",
    "\n",
    "# Create a network of the entities based on their co-occurrence in the same sentence\n",
    "entity_sets = []\n",
    "for sent in bookdoc.sents:\n",
    "    entities = set([ent for ent in sent.ents if ent.label_ == 'PERSON']) # KEY CHANGE HERE\n",
    "    if len(entities) > 1:\n",
    "        entity_sets.append(entities) \n",
    "\n",
    "# Each item in entity_sets is the group of entities that co-occur in the same sentence\n",
    "entity_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c11c50",
   "metadata": {},
   "source": [
    "## üß† Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac1357",
   "metadata": {},
   "source": [
    "1. Use the cosine similarity function to find the most similar documents to the first document in the df based on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4624a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sim = cosine_similarity(tfidf_df.iloc[0].values.reshape(1, -1), tfidf_df.iloc[1:].values)\n",
    "display(tfidf_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of top 5 most similar documents (excluding the first document itself)\n",
    "tfidf_top5_indices = tfidf_sim[0].argsort()[-5:][::-1]\n",
    "covid_df.iloc[tfidf_top5_indices]['webTitle']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af24df9",
   "metadata": {},
   "source": [
    "2. Use the cosine similarity function to find the most similar documents to the first document in the df based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2decfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "covid_embeddings = model.encode(covid_df['bodyContent'], convert_to_tensor=True)\n",
    "\n",
    "df = pd.DataFrame(covid_embeddings.cpu().numpy(), index=covid_df['webTitle'])\n",
    "\n",
    "embedding_sim = cosine_similarity(df.iloc[0].values.reshape(1, -1), df.iloc[1:].values)\n",
    "embedding_top5_indices = embedding_sim[0].argsort()[-5:][::-1]\n",
    "covid_df.iloc[embedding_top5_indices]['webTitle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfed8db",
   "metadata": {},
   "source": [
    "3. Compare the results of the two methods (plot as a scatter graph). Do they yield similar results? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a1d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(embedding_sim, tfidf_sim, alpha=0.1)\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"Embedding Similarity to First Document\")\n",
    "plt.ylabel(\"TF-IDF Similarity to First Document\")\n",
    "plt.title(\"Comparison of Embedding and TF-IDF Similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d025454",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b5878",
   "metadata": {},
   "source": [
    "1. Remove the `vectorizer_model=vectorizer_model` argument from the `BERTopic` constructor, and re-run the model. What happens to the topics?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684975a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for specific sections and drop rows with NaN in 'bodyContent'\n",
    "sample_df = df[df['sectionName'].isin(['Opinion', 'Football'])\n",
    "               ].dropna(subset='bodyContent').sample(n=1000, random_state=42) # Sample 1000 rows for speed today\n",
    "docs = sample_df['bodyContent'].tolist()\n",
    "\n",
    "topic_model = BERTopic() #  create BERTopic model\n",
    "topics, probs = topic_model.fit_transform(docs) # fit the model to the documents\n",
    "topic_info = topic_model.get_topic_info() # get topic information\n",
    "topic_info # display the topics found\n",
    "\n",
    "# We get basically the same topics, but the key words are undesirably full of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42001ab",
   "metadata": {},
   "source": [
    "2. Try selecting some articles from different sections of the Guardian, such \"Politics\", \"World news\", \"US news\". What do we find for topics here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for specific sections and drop rows with NaN in 'bodyContent'\n",
    "sample_df = df[df['sectionName'].isin([\"Politics\", \"World news\", \"US news\"])\n",
    "               ].dropna(subset='bodyContent').sample(n=1000, random_state=42) # Sample 1000 rows for speed today\n",
    "docs = sample_df['bodyContent'].tolist()\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\") # only used for c-TF-IDF stage\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model) #  create BERTopic model\n",
    "topics, probs = topic_model.fit_transform(docs) # fit the model to the documents\n",
    "topic_info = topic_model.get_topic_info() # get topic information\n",
    "topic_info # display the topics found\n",
    "\n",
    "# Clear separation of stories topics by section is not observed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b026455",
   "metadata": {},
   "source": [
    "## ü§ñ Applying State-of-the-Art NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db867ee8",
   "metadata": {},
   "source": [
    "1. We used the simple, fast, cheap `gpt-4o-mini` model. Try using the more powerful `gpt-4o` model instead on our labeled data. How does it perform? Can you think of a task (perhaps from your own work) where the more powerful model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_classify(article_text):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\", # UPDATED MODEL\n",
    "        input=[\n",
    "            {\n",
    "                # This part defines the system prompt, which sets the context for the model\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": system_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                # This part defines the user input, which is the article text to be classified\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": article_text\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        ],\n",
    "\n",
    "        # This next part defines the output format, a JSON object with a single key \"classification\"\n",
    "        text={\"format\": {\n",
    "            \"name\": \"news_article_classification\",\n",
    "            \"type\": \"json_schema\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"classification\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The classification output of the model: 1 for supportive of UK's approach to COVID, 0 otherwise.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"classification\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }},\n",
    "    )\n",
    "    return json.loads(response.output_text)['classification']\n",
    "\n",
    "# Limited opportunity for improvement in this classfication task\n",
    "# It takes a bit longer / is more expensive, so probably not worth it\n",
    "# However it may be suited to many other more complex tasks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
