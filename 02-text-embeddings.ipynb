{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0685081",
   "metadata": {},
   "source": [
    "# üß† Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396adff",
   "metadata": {},
   "source": [
    "## ‚Üí Simple document vectors\n",
    "\n",
    "### üî¢ Count / frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0075e2a",
   "metadata": {},
   "source": [
    "You will recall that we previously used token counts to represent text, and showed this for an example news headline corpus in a document-feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfa1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import count vectorizer and pandas to display the results\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "financial_headlines = [\n",
    "    \"Apple stocks soar to record highs as investors celebrate strong earnings\",\n",
    "    \"Global markets crash amid fears of a deepening recession and economic turmoil\",\n",
    "    \"Central bank raises interest rates, boosting confidence in economic recovery\",\n",
    "    \"Tesco reports impressive quarterly profits as retail sales surge\",\n",
    "    \"Oil prices collapse after oversupply announcement, sparking industry panic\",\n",
    "    \"Unemployment rate drops to lowest level in a decade as job market thrives\",\n",
    "    \"Debenhams files for bankruptcy protection as retail sector faces disaster\",\n",
    "    \"Investors optimistic as trade tensions ease and markets rally strongly\",\n",
    "    \"Housing market slows down as prices fall and buyers worry\",\n",
    "    \"Novo Nordisk faces lawsuit over drug safety, shares plunge on negative news\",\n",
    "    \"Consumer confidence reaches all-time high as retail sales boom\",\n",
    "    \"Tesla recalls thousands of vehicles over safety concerns, shares tumble\",\n",
    "    \"Earnings miss sends shares of Trump Inc plummeting as profits disappoint\",\n",
    "    \"Mergers and acquisitions activity hits new peak as companies celebrate growth\",\n",
    "    \"Cryptocurrency market rebounds after sharp decline, investors cheer recovery\",\n",
    "    \"Retail sales disappoint during holiday season as consumer confidence drops\",\n",
    "    \"Government stimulus package boosts economic outlook, markets surge\",\n",
    "    \"Manufacturing sector contracts for third straight month as demand weakens\",\n",
    "    \"Lab grown meat startup secures major funding round, industry hails innovation\",\n",
    "    \"Layoffs announced as company restructures operations, employees face uncertainty\"\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # load English tokenizer, POS tagger, etc.\n",
    "\n",
    "# Preprocess texts with spaCy (lemmatize, remove stopwords/punctuation)\n",
    "def spacy_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Use CountVectorizer with the spaCy tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer)\n",
    "X = vectorizer.fit_transform(financial_headlines)\n",
    "\n",
    "# Convert to dense matrix and view as DataFrame\n",
    "df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names_out()),\n",
    "                  index=financial_headlines)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c08f1",
   "metadata": {},
   "source": [
    "Each row is known as a vector that represents its corresponding document. The columns are the features, which in this case are the words in the vocabulary. Each cell in the matrix is a count of how many times that word appears in that document. We can numerically describe the documents in this way, but it is not so informative.\n",
    "\n",
    "\n",
    "### üìä TF-IDF\n",
    "\n",
    "We can, however, represent these texts in a different way. Firstly, we could normalise the counts to be between 0 and 1 to account for different length documents. Another method is to use a weighted representation, where the weights are based on the **importance** of the word in the document. This is known as term frequency-inverse document frequency (**TF-IDF**). The idea is that if a word appears frequently in a document but not in many other documents, it is likely to be important for that document. The TF-IDF score is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\cdot \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\text{TF}(t, d)$ is the term frequency of term $t$ in document $d$, which is the number of times $t$ appears in $d$ divided by the total number of terms in $d$.\n",
    "- $\\text{IDF}(t)$ is the inverse document frequency of term $t$, which is calculated as:\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{n_t}\\right)\n",
    "$$\n",
    "where:\n",
    "- $N$ is the total number of documents in the corpus.\n",
    "- $n_t$ is the number of documents containing term $t$.\n",
    "\n",
    "The TF-IDF score is a measure of how important a word is to a document in a corpus, taking into account the words in the other documents in the corpus. It is often used in information retrieval and text mining. The TF-IDF vector for a document describes it by a weighted representation of its important words.\n",
    "\n",
    "Let's see how we can calculate the TF-IDF score for our example news headlines. We will use the `TfidfVectorizer` class from the `sklearn` library to do this. The `TfidfVectorizer` class takes a list of documents as input and returns a matrix of TF-IDF scores for each document and each term in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use TfidfVectorizer with the spaCy tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "X = vectorizer.fit_transform(financial_headlines)\n",
    "\n",
    "# Convert to dense matrix and view as DataFrame\n",
    "df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names_out()),\n",
    "                  index=financial_headlines)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc002f5",
   "metadata": {},
   "source": [
    "TF-IDF provides a more informative representation of the text than simple counts as a way to describe a document. For example, common words in these headlines like \"market\" don't tell us much about the content of the document, so they receive a lower score (if they are present). Unique words in a document give us more information about the document, so they receive a higher score. This is useful for tasks like document classification, where we want to identify the topic of a document based on its content. This is perhaps more apparent when we have longer documents and a larger corpus. Consider the following examples of Guardian opinion pieces about COVID-19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24136969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "covid_df = pd.read_csv(\"data/covid_stories.csv\")\n",
    "display(covid_df)\n",
    "\n",
    "# Use TfidfVectorizer with the spaCy tokenizer on the COVID dataset\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "X = vectorizer.fit_transform(covid_df['bodyContent'])\n",
    "\n",
    "# Convert and view as DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=list(vectorizer.get_feature_names_out()),\n",
    "                  index=covid_df['webTitle'])\n",
    "\n",
    "display(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb49a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sorted TF-IDF vector for the first story\n",
    "print(tfidf_df.index[0])\n",
    "display(tfidf_df.iloc[0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169a7fd",
   "metadata": {},
   "source": [
    "The first row in the dataframe indicates the most important words in the first document, relative to the rest of the corpus. It is clearly focused on the outbreak itself in Wuhan. \"Coronavirus\", \"Covid-19\", etc. are terms likely to appear across most documents in the corpus, so whilst they might appear frequently in the document, they are not as important for distinguishing it from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6fb492",
   "metadata": {},
   "source": [
    "## üß† Embeddings\n",
    "\n",
    "The aformentioned methods of representing text are known as **bag-of-words** models. They are simple and effective, but they have some limitations. For example, they do not take into account the order of the words in the text, and they do not capture the meaning of the words. Without some dimensionality reduction attempts they are also too unwieldy to work with. This is where embeddings come in.\n",
    "\n",
    "In NLP, an embedding is a dense vector representation of a piece of text (could be a word or document) such that the vector captures the semantic \"meaning\" of the text. Words or documents that are similar in meaning should end up with vectors that are close to each other in this vector space. This idea underpins a lot of modern NLP ‚Äì instead of dealing with words as discrete symbols, we embed them in a continuous multidimensional vector space that a machine learning model can work with.\n",
    "\n",
    "* Word embeddings\n",
    "  * Map individual words to vectors. These models learn from large corpora such that words used in similar contexts end up near each other in the vector space (i.e. their vectors look similar). For example, ‚Äúking‚Äù and ‚Äúqueen‚Äù might be close in space, and analogies can be solved with vector arithmetic (classic example: king ‚Äì man + woman ‚âà queen). Word2Vec and GloVe are classic algorithms that produce word embeddings. More advanced models like BERT and GPT also produce contextualized word embeddings, meaning the same word can have different vectors depending on its context in a sentence.\n",
    "\n",
    "* Sentence / document embeddings\n",
    "  * Produce a vector for a whole sentence or larger text. This is often done by using models like BERT (a transformer) or specialized sentence embedding models (like Sentence Transformers) that aggregate meaning over multiple words. Sentence embeddings are useful to compare similarity of entire sentences or to feed documents into clustering or classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee1cac",
   "metadata": {},
   "source": [
    "### üí¨ Word embeddings\n",
    "\n",
    "spaCy has a built-in word embedding model that can be used to convert words into vectors. The `en_core_web_md` model contains pre-trained ('Explosion') word vectors that can be used to represent words in a continuous vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")  # load English tokenizer, POS tagger, etc. Note that we use a medium model here\n",
    "\n",
    "word1 = nlp(\"king\")\n",
    "word2 = nlp(\"queen\")\n",
    "word3 = nlp(\"man\")\n",
    "word4 = nlp(\"woman\")\n",
    "word5 = nlp(\"programming\")\n",
    "\n",
    "df = pd.concat([pd.Series(word1.vector, name='king'),\n",
    "                 pd.Series(word2.vector, name='queen'),\n",
    "                 pd.Series(word3.vector, name='man'),\n",
    "                 pd.Series(word4.vector, name='woman'),\n",
    "                 pd.Series(word5.vector, name='programming')],\n",
    "                axis=1).T\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659dbcf",
   "metadata": {},
   "source": [
    "We end up with a 300d vector for each word in the vocabulary. The vectors are dense, meaning they have non-zero values in many dimensions, and they capture semantic relationships between word.\n",
    "\n",
    "We use cosine similarity to measure the similarity between two word vectors - i.e. how similar the meanings of the words are based on their context in the training data. Cosine similarity is defined as the cosine of the angle between two vectors, and it ranges from -1 (completely dissimilar) to 1 (completely similar). It is calculated as follows:\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}\n",
    "$$\n",
    "where:\n",
    "- $A$ and $B$ are the two vectors.\n",
    "- $A \\cdot B$ is the dot product of the two vectors.\n",
    "- $||A||$ and $||B||$ are the magnitudes (lengths) of the vectors.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd69d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity (king‚Äìqueen):\", word1.similarity(word2))\n",
    "print(\"Similarity (king‚Äìman):\", word1.similarity(word3))\n",
    "print(\"Similarity (king‚Äìwoman):\", word1.similarity(word4))\n",
    "print(\"Similarity (queen‚Äìman):\", word2.similarity(word3))\n",
    "print(\"Similarity (queen‚Äìwoman):\", word2.similarity(word4))\n",
    "print(\"Similarity (king‚Äìprogramming):\", word1.similarity(word5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff555e",
   "metadata": {},
   "source": [
    "### üìÑ Document / sentence embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e147ae",
   "metadata": {},
   "source": [
    "A simple approach is to just average the word embeddings of all words in a sentence to get a single vector. This is known as **mean pooling**. However, more sophisticated methods like using attention mechanisms or transformers can yield better results (coming soon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"The economy is growing.\")\n",
    "doc2 = nlp(\"The market is expanding.\")\n",
    "doc3 = nlp(\"The weather is nice.\")\n",
    "doc4 = nlp(\"The economy is shrinking.\")\n",
    "doc5 = nlp(\"Scooby Doo where are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11663ff",
   "metadata": {},
   "source": [
    "We can still produce a document-feature matrix. However now the features are not words, but more abstract concepts. This makes them less directly interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.Series(doc1.vector, name='doc1'),\n",
    "                 pd.Series(doc2.vector, name='doc2'),\n",
    "                 pd.Series(doc3.vector, name='doc3'),\n",
    "                 pd.Series(doc4.vector, name='doc4'),\n",
    "                 pd.Series(doc5.vector, name='doc5')],\n",
    "                axis=1).T\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60dc88",
   "metadata": {},
   "source": [
    "Again, we can use cosine similarity to measure the similarity between two document vectors. This is useful for tasks like document clustering, where we want to group similar documents together based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ca6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity (doc1‚Äìdoc2):\", doc1.similarity(doc2))\n",
    "print(\"Similarity (doc1‚Äìdoc3):\", doc2.similarity(doc3))\n",
    "print(\"Similarity (doc1‚Äìdoc4):\", doc1.similarity(doc4))\n",
    "print(\"Similarity (doc1‚Äìdoc5):\", doc1.similarity(doc5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97122453",
   "metadata": {},
   "source": [
    "All of these similarities are quite high due to the mean pooling. Shared common words like \"the\", \"is\", can inflate the similarity scores, even if the sentences are not semantically similar. In addition, many unrelated sentences end up with vectors near the mean of the embedding space ‚Äî so the cosine similarity between them is often still high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54935648",
   "metadata": {},
   "source": [
    "A better approach is to use pre-trained models that are specifically designed to produce high-quality sentence / document embeddings. For this, we‚Äôll use the `sentence-transformers` library, which has pre-trained models that can convert sentences to vectors. One popular model is all-MiniLM-L6-v2 ‚Äì a MiniLM (small distilled transformer) that outputs 384-dimensional sentence embeddings and is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ede9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small, fast, decent quality\n",
    "\n",
    "# List of documents\n",
    "documents = [\n",
    "    \"The economy is growing.\",\n",
    "    \"The market is expanding.\",\n",
    "    \"The weather is nice.\",\n",
    "    \"The economy is shrinking.\",\n",
    "    \"Scooby Doo where are you?\"\n",
    "]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "# Check embedding shape\n",
    "print(embeddings.shape)  # torch.Size([5, 384])\n",
    "\n",
    "# Convert embeddings to a DataFrame for better visualization\n",
    "df = pd.DataFrame(embeddings.cpu().numpy(), index=documents)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c6c3f",
   "metadata": {},
   "source": [
    "Again, we can calculate the similarity between two document vectors using cosine similarity. This is useful for tasks like document clustering, where we want to group similar documents together based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(embeddings.cpu().numpy())\n",
    "# Convert to DataFrame for better visualization\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, index=documents, columns=documents)\n",
    "# Display the cosine similarity matrix\n",
    "display(cosine_sim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcc4f8",
   "metadata": {},
   "source": [
    "## ü™Ü Embedding models\n",
    "\n",
    "Different embedding models are designed for different tasks. Models can vary by size / speed, language support, training data, and task suitability. Researchers and companies create a model by training on large text corpora to predict next / missing / contextual words.\n",
    "\n",
    "It's not actually that hard to train or 'finetune' your own small model, it can even outperform larger models on specific tasks. However, this is often not necessary as there are many pre-trained models available that are performant out of the box on a wide range of domains. spaCy and sentence_transformers have several built-in, but check out [Hugging Face's Model Hub](https://huggingface.co/) for a wide range of models. You can use the `transformers` library to load and use these models in your own code.\n",
    "\n",
    "N.B. I'm referring to embeddings here, but the same applies to task-specific models for things like classification, translation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f3caa",
   "metadata": {},
   "source": [
    "### üèãÔ∏è Exercise\n",
    "1. Use the cosine similarity function to find the most similar documents to the first document in the df based on TF-IDF\n",
    "2. Use the cosine similarity function to find the most similar documents to the first document in the df based on embeddings\n",
    "3. Compare the results of the two methods (plot as a scatter graph). Do they yield similar results? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ee6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0796d39",
   "metadata": {},
   "source": [
    "## üåØ Wrap up\n",
    "\n",
    "We have seen how to represent text as vectors using TF-IDF and embeddings. These representations allow us to capture the meaning of the text (to different degrees) and compare documents based on their content. These representations underpin a wide range of NLP tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
